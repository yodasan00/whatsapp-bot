import type { OpenRouterCore } from '../core.js';
import type * as models from '../models/index.js';
import type { CallModelInput } from './async-params.js';
import type { RequestOptions } from './sdks.js';
import type { ResponseStreamEvent, InferToolEventsUnion, ParsedToolCall, StopWhen, Tool, ToolStreamEvent } from './tool-types.js';
export interface GetResponseOptions<TTools extends readonly Tool[]> {
    request: CallModelInput<TTools>;
    client: OpenRouterCore;
    options?: RequestOptions;
    tools?: TTools;
    stopWhen?: StopWhen<TTools>;
}
/**
 * A wrapper around a streaming response that provides multiple consumption patterns.
 *
 * Allows consuming the response in multiple ways:
 * - `await result.getText()` - Get just the text
 * - `await result.getResponse()` - Get the full response object
 * - `for await (const delta of result.getTextStream())` - Stream text deltas
 * - `for await (const msg of result.getNewMessagesStream())` - Stream incremental message updates
 * - `for await (const event of result.getFullResponsesStream())` - Stream all response events
 *
 * For message format conversion, use the helper functions:
 * - `toChatMessage(response)` for OpenAI chat format
 * - `toClaudeMessage(response)` for Anthropic Claude format
 *
 * All consumption patterns can be used concurrently thanks to the underlying
 * ReusableReadableStream implementation.
 *
 * @template TTools - The tools array type to enable typed tool calls and results
 */
export declare class ModelResult<TTools extends readonly Tool[]> {
    private reusableStream;
    private streamPromise;
    private textPromise;
    private options;
    private initPromise;
    private toolExecutionPromise;
    private finalResponse;
    private toolEventBroadcaster;
    private allToolExecutionRounds;
    private resolvedRequest;
    constructor(options: GetResponseOptions<TTools>);
    /**
     * Get or create the tool event broadcaster (lazy initialization).
     * Ensures only one broadcaster exists for the lifetime of this ModelResult.
     */
    private ensureBroadcaster;
    /**
     * Type guard to check if a value is a non-streaming response
     */
    private isNonStreamingResponse;
    /**
     * Initialize the stream if not already started
     * This is idempotent - multiple calls will return the same promise
     */
    private initStream;
    /**
     * Execute tools automatically if they are provided and have execute functions
     * This is idempotent - multiple calls will return the same promise
     */
    private executeToolsIfNeeded;
    /**
     * Internal helper to get the text after tool execution
     */
    private getTextInternal;
    /**
     * Get just the text content from the response.
     * This will consume the stream until completion, execute any tools, and extract the text.
     */
    getText(): Promise<string>;
    /**
     * Get the complete response object including usage information.
     * This will consume the stream until completion and execute any tools.
     * Returns the full OpenResponsesNonStreamingResponse with usage data (inputTokens, outputTokens, cachedTokens, etc.)
     */
    getResponse(): Promise<models.OpenResponsesNonStreamingResponse>;
    /**
     * Stream all response events as they arrive.
     * Multiple consumers can iterate over this stream concurrently.
     * Preliminary tool results are streamed in REAL-TIME as generator tools yield.
     */
    getFullResponsesStream(): AsyncIterableIterator<ResponseStreamEvent<InferToolEventsUnion<TTools>>>;
    /**
     * Stream only text deltas as they arrive.
     * This filters the full event stream to only yield text content.
     */
    getTextStream(): AsyncIterableIterator<string>;
    /**
     * Stream incremental message updates as content is added in responses format.
     * Each iteration yields an updated version of the message with new content.
     * Also yields OpenResponsesFunctionCallOutput after tool execution completes.
     * Returns ResponsesOutputMessage or OpenResponsesFunctionCallOutput compatible with OpenAI Responses API format.
     */
    getNewMessagesStream(): AsyncIterableIterator<models.ResponsesOutputMessage | models.OpenResponsesFunctionCallOutput>;
    /**
     * Stream only reasoning deltas as they arrive.
     * This filters the full event stream to only yield reasoning content.
     */
    getReasoningStream(): AsyncIterableIterator<string>;
    /**
     * Stream tool call argument deltas and preliminary results.
     * Preliminary results are streamed in REAL-TIME as generator tools yield.
     * - Tool call argument deltas as { type: "delta", content: string }
     * - Preliminary results as { type: "preliminary_result", toolCallId, result }
     */
    getToolStream(): AsyncIterableIterator<ToolStreamEvent<InferToolEventsUnion<TTools>>>;
    /**
     * Get all tool calls from the completed response (before auto-execution).
     * Note: If tools have execute functions, they will be automatically executed
     * and this will return the tool calls from the initial response.
     * Returns structured tool calls with parsed arguments.
     */
    getToolCalls(): Promise<ParsedToolCall<TTools[number]>[]>;
    /**
     * Stream structured tool call objects as they're completed.
     * Each iteration yields a complete tool call with parsed arguments.
     */
    getToolCallsStream(): AsyncIterableIterator<ParsedToolCall<TTools[number]>>;
    /**
     * Cancel the underlying stream and all consumers
     */
    cancel(): Promise<void>;
}
//# sourceMappingURL=model-result.d.ts.map